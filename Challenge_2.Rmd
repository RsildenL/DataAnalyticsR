---
title: "Challenge 2: Introduction to Probability Models"
subtitle: "Data Analytics with R"
author: "Rebecca Silden Langlo, Afonso Penalva, Kevin Ganglbauer, Henrik Mülheims, Martí Pérez & Karmanpreet Singh" 
date: "22/02/2021"
      
output: 
  html_document:
    code_folding: hide
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: no
      smooth_scroll: yes
    number_sections: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(data.table)
library(MLmetrics)
library(caret)
```

Predicting Machine Failure

# Creating the relevant variable for the model
```{r Read the datafiles, cache = TRUE}
#machine_data <- fread("Data/machine_data.csv")
#product_data <- fread("Data/product_data.csv")
transaction_data <- fread("Data/transactional_data.csv")
transaction_data$date <- as.Date(transaction_data$date)
machine_failures <- fread("Data/machine_failures.csv")
```


## 1. Merging datasets 
Merge the transactional dataset with the machine failures data set setting failure variable to 0 when no failure is recorded.

```{r merging data}
merged_data <- merge(transaction_data, machine_failures, 
                     by = c("machine", "column", "timestamp"), 
                     all.x = TRUE)

merged_data[is.na(failure), failure :=0]
summary(merged_data)
dim(merged_data)
```
Merging the two datasets results in a 1,840,477 observations of 5 features, which will be used in the following computations. Failure is 1 on the first sale of a machine after a broken period, and 0 otherwise. An initial inspection of the features shows that the dataset is highly imbalanced regarding observed failures, with a mean of only 0.006. We can also see from the date feature that we have observation from 2017-01-02 until 2017-04-01. 

## 2. Create "last_vend" variable
In the transactional data table, create a variable called “last_vend” containing the timestamp of the previous sale of each machine
Hint: Remember you can use the function “shift” once the data is ordered according to machine and date with function “order” i.e. dt = dt[order(x,y)] where x and y are column names of dt
```{r last_vend}
merged_data <- merged_data[order(machine, timestamp)]
merged_data[, last_vend := shift(timestamp, 1), by=.(machine)]  #Rebecca

#merged_data$last_vend <- shift(merged_data$timestamp, n = 1 )  ## Karman 
#merged_data<- merged_data[-1,] ## Karman
```
A last_vend variable, containing the timestamp of the previous sale of each machine, using the order and shift functions. For the first observations of each of the machines, an the last_vend is reported as NA.

REBECCA - Solved weird stuff between machines using by = machine (fill = 0 first obs with 0's)
KARMAN-Order(machine, timestamp) instead of (machine, date)
KARMAN-Changed the n=-1 to n=1, removed first row [I think we still need to delete every first row of each machine]

## 3. Create "deltahours" variable
Create a new variable in the transactional data table called “deltahours” containing, for every sale, the hours that passed since the last sale.

# Check datatypes (deltahours only one in hours format)

```{r deltahours, cache = TRUE}
## there will be some weird diffhours btw. the machines (include a by/if function?)
merged_data$deltahours <- difftime(merged_data$timestamp, merged_data$last_vend, units = "hours")
#merged_data[!is.na(last_vend), deltahours := difftime(merged_data$timestamp, merged_data$last_vend, units = "hours"), by=machine]
#merged_data[, deltahours := ifelse(is.na(last_vend)==TRUE, "NA", difftime(merged_data$timestamp, merged_data$last_vend, units = "hours")), by=machine]
#merged_data[, 'deltahours' := as.numeric(difftime(timestamp, last_vend, units = 'hours'))] ##Afonso
summary(merged_data$deltahours)
```



## 4. Create auxiliary table called "machine_daily_average"
Create an auxiliary data table called “machine_daily_average” with the average daily sales per machine. 
Use this auxiliary table to attach to every row of the transactional data table the the average daily sales per machine. You can do this by doing a merge.
```{r}
### Check which one is correct
daily_sales <- merged_data[ , .(items_machine_day = uniqueN(timestamp)), by = .(machine, date)]
machine_daily_average <- daily_sales[ ,.(avg_daily_sales = mean(items_machine_day)), by = machine]
#machine_daily_average <- merged_data[ ,.(avg_daily_sales = .N), by = machine]
machine_daily_average

##Afonso
#machine_daily_average <- merged_data[, avg_daily_sales := mean(.N), by = .(machine, date)] 
#machine_daily_average <- machine_daily_average[, c('machine', 'date', 'avg_daily_sales')]
#machine_daily_average <- machine_daily_average[, mean_daily_sales := mean(avg_daily_sales), by = c('machine', 'date')]
#machine_daily_average <- machine_daily_average[, mean(mean_daily_sales), by = machine]
#merged_data <- merge(merged_data, machine_daily_average, by = 'machine', all.x=TRUE)
```

```{r}
merged_data$daily_sales_machine <- merge(merged_data, machine_daily_average, 
                                           by = "machine", all = TRUE)$avg_daily_sales
head(merged_data)
```


## 5. Create "delta" variable 
Create a new variable called “delta” in the transactional data table containing a normalized version of deltahours consisting on the deltahours associated with each sale divided by the average deltahours of each machine i.e. delta = deltahours/(24/daily_sales_machine). The interpretation of delta is the amount of “missed sales” if the machine was selling at a constant rate

```{r}
##(24/daily_sales_machine) tells us that on an average this much time is spent between buying two consecutive items. So dividing deltahours with this will tell us how was our specific purchase frequency compared to average purchase frequency of that particular machine 
merged_data$delta <- merged_data[ ,.(delta = as.numeric(deltahours/(24/daily_sales_machine))),] #changed delta to numeric
head(merged_data)    
```

(24/daily_sales_machine) tells us that on an average this much time is spent between buying two consecutive items. So dividing deltahours with this will tell us how was our specific purchase frequency compared to average purchase frequency of that particular machine.

# Creating the model 

## 6. Linear logistic regression 
Select 30% of the machines in the transactional data for testing and 70% of the machines for training and train a linear logistic regression model called “m” to predict whether a machine has a failure as a function of variable delta. What is the value of the intercept and the coefficient accompanying variable delta?

Test train split: 
```{r}
merged_data[is.na(merged_data)] <- 0
```

```{r}
set.seed(12)
index <- sample(x=1:2, size=nrow(merged_data), replace = TRUE, prob=c(0.7, 0.3))
train <- merged_data[index == 1, ] #merged_data$machine
test <- merged_data[index == 2, ]

prop.table(table(train$failure))
prop.table(table(test$failure))
```
** Highly inbalanced dataset (keeping similar distribution in test and train set)

Logistic regression: 
```{r}
m <- glm(failure ~ delta,  data = train, family = "binomial",  na.action = na.omit)
m_predictions <- predict(m, newdata = test, type = "response")
#pred_train <- predict(m, train, type = 'response')
#pred_test <- predict(m, test, type = 'response')
summary(m)$coefficients
summary(m)
```


### a) AUC 
What’s the AUC, a measure of quality, of the model you have built on the train set? and on test set?
```{r}
#### DUE TO THE SHIFT -- need to remove either first or last observation
##UPDATE:Removed observation, but still this problem
AUC(m$fitted.values, train$failure)
AUC(m_predictions, test$failure)

#auc_train <- auc(train$failure, pred_train)
#auc_test <- auc(test$failure, pred_test)
```


### b) Plot
Plot the function of probability of failure with respect to delta to gain intuition
```{r}
range(merged_data$delta, na.rm = TRUE) 
train$delta <- as.numeric(train$delta) 

b0 <- m$coefficients[1]
X1 <- m$coefficients[2]

typeof(m$coefficients)
prob = function(delta) {
  result <- 1/(1+exp(-(b0 + X1*delta)))
  print(result)
}
curve(prob, from = 0, to = 30, n = 500, 
      xlab = "Delta", 
      ylab = "Probability of Failure")

#prob = 1/(1+exp(-(b0 + X1*merged_data$delta)
# Possible explanation: A delta of 0-6 hours is somehow expected as traffic is significantly reduced during nighttime. Any longer non-functioning time of a machine is most probably signaling a failure, so the probability spikes.
```

```{r}
#plot(failure ~ delta, data = merged_data)

#prob  <- 1/(1+exp(-(summary(m)$coefficients[1] + summary(m)$coefficients[2]*as.numeric(merged_data$delta))))
#curve(expr = (1/(1+exp(-(summary(m)$coefficients[1] + summary(m)$coefficients[2]*x)))), x = merged_data$delta)

```


### c) Alarms 
#i)
```{r}
f = function(x){1/(1+exp(-(-6.921262 + 0.563285*x)))}-0.6
delta_med <- uniroot(f, c(0, 25))$root

g = function(x){1/(1+exp(-(-6.921262 + 0.563285*x)))}-0.8
delta_high <- uniroot(g, c(0, 25))$root

```

#ii)
```{r}
merged_data[, 'med_risk' := ifelse(delta >= delta_med, 1, 0)]
merged_data[, 'high_risk' := ifelse(delta >= delta_high, 1, 0)]

average_per_day_med <- sum(merged_data$med_risk, na.rm=T)/uniqueN(merged_data$date)
average_per_day_high <- sum(merged_data$high_risk, na.rm=T)/uniqueN(merged_data$date)

```

#iii)
```{r}
merged_data[med_risk == 1, (1-mean(failure)),]
merged_data[high_risk == 1, (1-mean(failure)),]
```

### d) Profit impact
# In this exercise we will estimate the profit impact of our EWS system vs the current system:
```{r}
# First, we filter by the instances that we classified as values with at least a medium risk of failure.
dt_alarm_med = merged_data[delta > delta_med]
dt_alarm_high = merged_data[delta > delta_high]# Assumption: Filter both medium and high risk because high risk instances imply having a medium risk of failure as well.

annual_perc_incr <- function(x, delta_x) {
conversion_factor <- 24/x$daily_sales_machine
x[, threshold_hours:=delta_x*conversion_factor]
x[, threshold_hours_fixed := threshold_hours + 1.5]  # The 1.5 stands for the average time it takes to fix a machine.
x[, delta_fixed:= threshold_hours_fixed * (1/(24/daily_sales_machine))]
x[, won_sales := failure*(delta-delta_fixed)]
add_rev = sum(x$won_sales*1.7) # Additional revenue of the EWS for at least medium risk alarms.
sys_cost = 10*(nrow(x[failure == 0])) # Cost of the system
vendex_cost_prev = uniqueN(merged_data$machine)*2.2*10 # Cost of current system per year (2.2 false alarm per machine per YEAR)
annual_profit_incr = 4*(add_rev - vendex_cost_prev) - sys_cost # As we are only given data from Jan 1st to April 1st we have to multiply our estimated profit increase by 4 to obtain the actual annual profit increase.
perc_incr = (annual_profit_incr/(nrow(merged_data)*1.7*4))*100 # Again, we have to multiply the denominator by 4 so that we receive the annual profit increase in percentage.
return(perc_incr)
}

## i. If we set the EWS only with the med-risk alarms, what is the annual profit we will generate vs the current system as a % of the total profit? [For simplicity, consider the total profit to be the margin per item times the number of items in the period]
annual_perc_incr(dt_alarm_med, delta_med)

## ii. And if we set the EWS only with the high-risk alarms?
annual_perc_incr(dt_alarm_high, delta_high)
```
If we configure an EWS for medium risk alarms instead of relying on the current system we estimate an increase in revenue of 0.3895761%.
Applying an EWS for high risk alarms only we forecast an increase in revenue of 0.2217171%.