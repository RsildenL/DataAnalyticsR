---
title: "Challenge 2: Introduction to Probability Models"
subtitle: "Data Analytics with R"
author: "Rebecca Silden Langlo, Afonso Penalva, Kevin Ganglbauer, Henrik Mülheims, Martí Pérez & Karmanpreet Singh" 
date: "22/02/2021"
      
output: 
  html_document:
    code_folding: hide
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: no
      smooth_scroll: yes
    number_sections: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(data.table)
library(MLmetrics)
library(caret)
```

Predicting Machine Failure

# Creating the relevant variable for the model
```{r Read the datafiles, cache = TRUE}
#machine_data <- fread("Data/machine_data.csv")
#product_data <- fread("Data/product_data.csv")
transaction_data <- fread("Data/transactional_data.csv")
transaction_data$date <- as.Date(transaction_data$date)
machine_failures <- fread("Data/machine_failures.csv")
```


## 1. Merging datasets 
Merge the transactional dataset with the machine failures data set setting failure variable to 0 when no failure is recorded.

```{r merging data}
merged_data <- merge(transaction_data, machine_failures, 
                     by = c("machine", "column", "timestamp"), 
                     all.x = TRUE)

merged_data[is.na(failure), failure :=0]
summary(merged_data)
dim(merged_data)
```
Merging the two datasets results in a 1,840,477 observations of 5 features, which will be used in the following computations. Failure is 1 on the first sale of a machine after a broken period, and 0 otherwise. An initial inspection of the features shows that the dataset is highly imbalanced regarding observed failures, with a mean of only 0.006. We can also see from the date feature that we have observation from 2017-01-02 until 2017-04-01. 

## 2. Create "last_vend" variable
In the transactional data table, create a variable called “last_vend” containing the timestamp of the previous sale of each machine
Hint: Remember you can use the function “shift” once the data is ordered according to machine and date with function “order” i.e. dt = dt[order(x,y)] where x and y are column names of dt
```{r last_vend}
merged_data <- merged_data[order(machine, timestamp)]
merged_data[, last_vend := shift(timestamp, 1), by=machine]  #Rebecca

#merged_data$last_vend <- shift(merged_data$timestamp, n = 1 )  ## Karman 
#merged_data<- merged_data[-1,] ## Karman
```
A last_vend variable, containing the timestamp of the previous sale of each machine, using the order and shift functions. For the first observations of each of the machines, an the last_vend is reported as NA.

REBECCA - Solved weird stuff between machines using by = machine (fill = 0 first obs with 0's)
KARMAN-Order(machine, timestamp) instead of (machine, date)
KARMAN-Changed the n=-1 to n=1, removed first row [I think we still need to delete every first row of each machine]

## 3. Create "deltahours" variable
Create a new variable in the transactional data table called “deltahours” containing, for every sale, the hours that passed since the last sale.
```{r deltahours, cache = TRUE}
## there will be some weird diffhours btw. the machines (include a by/if function?)
merged_data$deltahours <- difftime(merged_data$timestamp, merged_data$last_vend, units = "hours")
#merged_data[!is.na(last_vend), deltahours := difftime(merged_data$timestamp, merged_data$last_vend, units = "hours"), by=machine]
#merged_data[, deltahours := ifelse(is.na(last_vend)==TRUE, "NA", difftime(merged_data$timestamp, merged_data$last_vend, units = "hours")), by=machine]

summary(merged_data$deltahours)
```



## 4. Create auxiliary table called "machine_daily_average"
Create an auxiliary data table called “machine_daily_average” with the average daily sales per machine. 
Use this auxiliary table to attach to every row of the transactional data table the the average daily sales per machine. You can do this by doing a merge.
```{r}
### Check which one is correct
daily_sales <- merged_data[ , .(items_machine_day = uniqueN(timestamp)), by = .(machine, date)]
machine_daily_average <- daily_sales[ ,.(avg_daily_sales = mean(items_machine_day)), by = machine]
#machine_daily_average <- merged_data[ ,.(avg_daily_sales = .N), by = machine]
machine_daily_average
```

```{r}
merged_data$daily_sales_machine <- merge(merged_data, machine_daily_average, 
                                           by = "machine", all = TRUE)$avg_daily_sales
head(merged_data)
```


## 5. Create "delta" variable 
Create a new variable called “delta” in the transactional data table containing a normalized version of deltahours consisting on the deltahours associated with each sale divided by the average deltahours of each machine i.e. delta = deltahours/(24/daily_sales_machine). The interpretation of delta is the amount of “missed sales” if the machine was selling at a constant rate

```{r}
##(24/daily_sales_machine) tells us that on an average this much time is spent between buying two consecutive items. So dividing deltahours with this will tell us how was our specific purchase frequency compared to average purchase frequency of that particular machine 
merged_data$delta <- merged_data[ ,.(delta = deltahours/(24/daily_sales_machine)),]
head(merged_data)    
```

(24/daily_sales_machine) tells us that on an average this much time is spent between buying two consecutive items. So dividing deltahours with this will tell us how was our specific purchase frequency compared to average purchase frequency of that particular machine 

# Creating the model 

## 6. Linear logistic regression 
Select 30% of the machines in the transactional data for testing and 70% of the machines for training and train a linear logistic regression model called “m” to predict whether a machine has a failure as a function of variable delta. What is the value of the intercept and the coefficient accompanying variable delta?

Test train split: 
```{r}
merged_data[is.na(merged_data)] <- 0
```

```{r}
set.seed(12)
index <- sample(x=1:2, size=nrow(merged_data), replace = TRUE, prob=c(0.7, 0.3))
train <- merged_data[index == 1, ]
test <- merged_data[index == 2, ]

prop.table(table(train$failure))
prop.table(table(test$failure))
```
** Highly inbalanced dataset (keeping similar distribution in test and train set)

Logistic regression: 
```{r}
m <- glm(failure ~ delta,  data = train, family = "binomial",  na.action = na.omit)
m_predictions <- predict(m, newdata = test, type = "response")
summary(m)$coefficients
summary(m)
```


### a) AUC 
What’s the AUC, a measure of quality, of the model you have built on the train set? and on test set?
```{r}
#### DUE TO THE SHIFT -- need to remove either first or last observation
##UPDATE:Removed observation, but still this problem
AUC(m$fitted.values, train$failure)
AUC(m_predictions, test$failure)
```


### b) Plot
Plot the function of probability of failure with respect to delta to gain intuition
```{r}
range(merged_data$delta, na.rm = TRUE) 
train$delta <- as.numeric(train$delta) 

b0 <- m$coefficients[1]
X1 <- m$coefficients[2]

typeof(m$coefficients)
prob = function(delta) {
  result <- 1/(1+exp(-(b0 + X1*delta)))
  print(result)
}
curve(prob, from = 0, to = 30, n = 500, 
      xlab = "Delta", 
      ylab = "Probability of Failure")

#prob = 1/(1+exp(-(b0 + X1*merged_data$delta)
# Possible explanation: A delta of 0-6 hours is somehow expected as traffic is significantly reduced during nighttime. Any longer non-functioning time of a machine is most probably signaling a failure, so the probability spikes.
```

```{r}
#plot(failure ~ delta, data = merged_data)

#prob  <- 1/(1+exp(-(summary(m)$coefficients[1] + summary(m)$coefficients[2]*as.numeric(merged_data$delta))))
#curve(expr = (1/(1+exp(-(summary(m)$coefficients[1] + summary(m)$coefficients[2]*x)))), x = merged_data$delta)

```


### c) Alarms 


### d) Profit impact


