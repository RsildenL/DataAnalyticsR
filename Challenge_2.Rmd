---
title: "Challenge 2: Introduction to Probability Models"
subtitle: "Data Analytics with R"
author: "Rebecca Silden Langlo, Afonso Penalva, Kevin Ganglbauer, Henrik Mülheims, Martí Pérez & Karmanpreet Singh" 
date: "22/02/2021"
      
output: 
  html_document:
    code_folding: hide
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: no
      smooth_scroll: yes
    number_sections: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(data.table)
library(MLmetrics)
library(caret)
```

Predicting Machine Failure

# Creating the relevant variable for the model
```{r Read the datafiles, cache = TRUE}
#machine_data <- fread("Data/machine_data.csv")
#product_data <- fread("Data/product_data.csv")
transaction_data <- fread("Data/transactional_data.csv")
transaction_data$date <- as.Date(transaction_data$date)
machine_failures <- fread("Data/machine_failures.csv")
```


## 1. Merging datasets 
Merge the transactional dataset with the machine failures data set setting failure variable to 0 when no failure is recorded.

```{r merging data}
merged_data <- merge(transaction_data, machine_failures, 
                     by = c("machine", "column", "timestamp"), 
                     all.x = TRUE)

merged_data[is.na(failure), failure :=0]
summary(merged_data)
```
Merging the two datasets results in a 1,840,477 observations of 5 features, which will be used in the following computations. Failure is 1 on the first sale of a machine after a broken period, and 0 otherwise.

** comment on inbalance for failure (more not failure than failure)

## 2. Create "last_vend" variable
In the transactional data table, create a variable called “last_vend” containing the timestamp of the previous sale of each machine
Hint: Remember you can use the function “shift” once the data is ordered according to machine and date with function “order” i.e. dt = dt[order(x,y)] where x and y are column names of dt
```{r last_vend}

merged_data <- merged_data[order(machine, date)]
merged_data$last_vend <- shift(merged_data$timestamp, n = -1 )

```


## 3. Create "deltahours" variable
Create a new variable in the transactional data table called “deltahours” containing, for every sale, the hours that passed since the last sale
```{r deltahours}
## there will be some weird diffhours btw. the machines (include a by/if function?)
merged_data$deltahours <- difftime(merged_data$last_vend, merged_data$timestamp, units = "hours")
summary(merged_data$deltahours)
```



## 4. Create auxiliary table called "machine_daily_average"
Create an auxiliary data table called “machine_daily_average” with the average daily sales per machine. 
Use this auxiliary table to attach to every row of the transactional data table the the average daily sales per machine. You can do this by doing a merge.
```{r}
### take a second look at this (KARMAN)
daily_sales <- merged_data[ , .(items_machine_day = uniqueN(timestamp)), by = .(machine, date)]
machine_daily_average <- daily_sales[ ,.(avg_daily_sales = mean(items_machine_day)), by = machine]
machine_daily_average
```

```{r}
merged_data$daily_sales_machine <- merge(merged_data, machine_daily_average, 
                                           by = "machine", all = TRUE)$avg_daily_sales
head(merged_data)
```


## 5. Create "delta" variable 
Create a new variable called “delta” in the transactional data table containing a normalized version of deltahours consisting on the deltahours associated with each sale divided by the average deltahours of each machine i.e. delta = deltahours/(24/daily_sales_machine). The interpretation of delta is the amount of “missed sales” if the machine was selling at a constant rate

```{r}
merged_data$delta <- merged_data[ ,.(delta = deltahours/(24/daily_sales_machine)),]
head(merged_data)    
```


# Creating the model 

## 6. Linear logistic regression 
Select 30% of the machines in the transactional data for testing and 70% of the machines for training and train a linear logistic regression model called “m” to predict whether a machine has a failure as a function of variable delta. What is the value of the intercept and the coefficient accompanying variable delta?

Test train split: 
```{r}
set.seed(12)
index <- sample(x=1:2, size=nrow(merged_data), replace = TRUE, prob=c(0.7, 0.3))
train <- merged_data[index == 1, ]
test <- merged_data[index == 2, ]

prop.table(table(train$failure))
prop.table(table(test$failure))
```
** Highly inbalanced dataset (keeping similar inbalance in test and train set)

Logistic regression: 
```{r}
m <- glm(failure ~ delta,  data = train, family = "binomial")
m_predictions <- predict(m, newdata = test, type = "response")
summary(m)$coefficients

```


### a) AUC 
What’s the AUC, a measure of quality, of the model you have built on the train set? and on test set?
```{r}
#### DUE TO THE SHIFT -- need to remove either first or last observation 
AUC(m$fitted.values, train$failure)  ### WHY are they not the same lenght??
AUC(m_predictions, test$failure)
```


### b) Plot
Plot the function of probability of failure with respect to delta to gain intuition
```{r}

```


### c) Alarms 


### d) Profit impact


